# Service

- pods are dynamic, means they can come and terminated in the cluster for example when using RC, pods are terminated and created during scaling operations

- when using Deployments, when updating the image version, pods are terminate and new pods take the place of older pods.

- so pods should be accessed via a service

- `service` is the logical-bridge between the pods and other services or end-users

- creating a service will create an endpoint for your pod(s):

  - `ClusterIp`: virtual Ip reachable inside the cluster

  - `NodePort`: a port that is the same on each node that is also reachable externally

  - `LoadBalancer`: created by cloud-provider that will route external traffic to every node on the NodePort (ELB on AWS)

- there is also possibility to use `DNS names`:

  - external-name can provide a DNS name for the service like service discovery useing DNS.

  - only works when `DNS add-on` is enabled

- by default service can only run between ports 30000-32767 but you could change this behavior by addng the --service-node-port-range argument to the `kube-apiserver` (init script of master node)

- service will automatically assign a random port to the NodePort but you can assign port manually by `nodePort` option (handle collisions manually).

- we shoukd use selector in in service to filter and reach appropriate pods.

- when we describe the service, there are 2 important sections:
  - IP: is the actual ClusterIP of the service inside the cluster
  - Endpoints: is the actual Pod IPs to access them directly and the service can loadbalance between them.

## why using pods?

- pods are like a small machine with their ownvirtual host which containers inside them can talk to each other via localhost.

- 1 reason is the port conflict with the containers, with pods the cluster will assign an IP address to each pod and the mapping between them can occur without any conflict

- CRI level is on pod level, so if we change the CRI from docker to CRI-O then no change is needed in the cluster and it should occur in pod level.

## exposing kubernetes ports

- that's really taking your set of deployments, your pods and creating a persistant endpoint that can talk to them.

- we can decide to expose them internally in cluster or externally to outside sources (means allow them accept connections)

- `kubectl expose`: creats a service for existing pods

> `service` is an endpoint that is consistant and anything inside or outside the cluster can access it

- when you create pods in kubernetes, they don't automatically get a DNS name for external connectivity with IP address.

- if we want to connect to pods, we need a service on top of existing pod.

- `CoreDNS`: allows us to resolve services by name.

- types of services are: `ClusterIp`, `NodePort`, `LoadBalancer` and `ExternalName`.

- `ClusterIp`< `NodePort`< `LoadBalancer`: means by creating a high level, the bottoms will be created by that one.

## kubernetes service DNS (CoreDNS basics)

- when you are in a container and cURL service name, you get back the response so there's a DNS server in there doing sth to match services to DNS names, believe it or not DNS is optional as a service or an add on inside your kubernetes cluster.

- internal DNS is provided by CoreDNS means when you create a service, you get the hostname that matches the service

### namespaces

- an orginazational parameter or a virtual wall inside the same cluster to avoid name clashes between pods, services, deployments inide it.

- in the same namespace, to access services: `curl <hostname>`

- in different namespace, to access services: `curl <hostname>.<namespace>.svc.cluster.local`

### ClusterIp (default)

> it's only available in the cluster (only good in cluster) so it's about one set of kubernetes pods talking to another set of pods and it gets its own DNS stress and that's going to be the DNS address in the core DNS control plane so it's going to get an IP address in virtual IP address space inside the cluster.

``` zsh
# start minikube cluster
minikube start

# start http-server
kubectl create deployment httpenv --image=bretfisher/httpenv

# 
kubectl scale deployment/httpenv --replicas=5

# create the service
kubectl expose deployment/httpenv --port 8888

# generator is the type of template we want the run cmd to use
kubectl run --generator run-pod/v1 tmp-shell --rm -it --image bretfisher/netshoot -- bash

# curl will hits one of the pods and returns its information
# curl [service-ip]:[service-port]
bash-5.0# curl httpenv:8888

# if you are on linux, as you know all nodes and containers by default can talk to
# each other, because your local-machine (node) has access to private ip addresses
# focus that your host don't know the DNS name like what we did inside pod (above-cmd)
# because the name-resolution is inclusive to be inside cluster
curl [service-cluster-IP]:[service-port]
```

- focus that our own `ClusterIP` is inside the cluster for node inside it to be able access it.

- the service name that we created becomes part of the DNS name for this service

- to summerize we create virtual IP inside the cluster for others to access it with friendly service name (here httpenv)

### NodePort

> designed for outside of cluster to talk to the service through the IP addresses on the nodes themselves.
>
> it allocates `high port` to each port and the port is open on every node's IP and anyone can connect (if they can reach node) and other pods need to be updated to this port.

- lets create IP that's exposed externally on host-IP

- by creating `NodePort`, it created a `ClusterIP` because it's how it connect and it redirect the high port to that `ClusterIP` for that service

``` bash
# create NodePort service (default type is ClusterIP)
kubectl expose deployment/httpenv --port 8888 --name httpenv-np --type NodePort

# by curling this high port, it hits to one of our pods
curl localhost:32270

# delete service
kubectl delete service/httpenv-np service/httpenv
```

- when get services, you see the port section is sth like: `8888:32270/TCP` the port on the left is one inside the cluster and container itself that's listening on and port on right is port on nodes exposed to the outside.

- external port range is: [30000,32767], as you can see it's a high port range, the hope is that there's no conflict

### LoadBalancer

> mostly used in cloud, it controls a LB endpoint external to the cluster and only available when infra provider gives you a LB.
>
> it creates `NodePort` and `ClusterIp` services, tells LB to send to `NodePort`

- it isn't built in by default and the only way to use it is through an external service usually in cloud service and it's a plugin in your kubernetes vendor.

``` bash
# create LoadBalancer service (default type is ClusterIP)
kubectl expose deployment/httpenv --port 8888 --name httpenv-lb --type LoadBalancer

# by curling this high port, it hits to one of our pods
curl localhost:8888

# delete service
kubectl delete service/httpenv-lb deployment/httpenv
```

- when get `LoadBalancer`, you see the port section is sth like: `8888:32250/TCP` and that's because it will pass localhost:8888 `LoadBalancer` traffic to `NodePort` localhost:32250.

### ExternalName

> adds `CNAME DNS` record to `CoreDNS` only and not used for pods but for giving pods a DNS name to use for sth outside kubernetes
